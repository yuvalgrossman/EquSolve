{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HASY_trainer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNO0PVGLcHc4ickIv90TLR6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvalgrossman/EquSolve/blob/master/Classifier/HASY_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAqqBRMGYo0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "bae0f14b-d1e6-4ed6-8d3d-2af6dd46e651"
      },
      "source": [
        "import os\n",
        "%cd '/content/'\n",
        "if not os.path.isdir('EquSolve'):\n",
        "  !git clone http://github.com/yuvalgrossman/EquSolve\n",
        "  %cd EquSolve\n",
        "else:\n",
        "  %cd EquSolve\n",
        "  !git pull\n",
        "\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "from tqdm import tqdm\n",
        "import webbrowser\n",
        "import time\n",
        "import pdb\n",
        "\n",
        "# project classes:\n",
        "from Classifier.HASYDataset import HASYDataset\n",
        "from Classifier.Net import Net\n",
        "from Utils.mapper import mapper"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'EquSolve'...\n",
            "warning: redirecting to https://github.com/yuvalgrossman/EquSolve/\n",
            "remote: Enumerating objects: 229, done.\u001b[K\n",
            "remote: Counting objects: 100% (229/229), done.\u001b[K\n",
            "remote: Compressing objects: 100% (183/183), done.\u001b[K\n",
            "remote: Total 229 (delta 103), reused 104 (delta 37), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (229/229), 21.06 MiB | 4.07 MiB/s, done.\n",
            "Resolving deltas: 100% (103/103), done.\n",
            "/content/EquSolve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdLVN95gtHvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, config, transform):\n",
        "        self.config = config\n",
        "        self.device = self.get_device()\n",
        "        self.transform = transform\n",
        "\n",
        "        #create dir to save train results:\n",
        "        theTime = \"{date:%Y-%m-%d_%H-%M-%S}\".format(date=datetime.datetime.now())\n",
        "        self.Train_Results_Dir = 'Classifier/TrainResults/Train_Results_' + theTime\n",
        "        os.mkdir(self.Train_Results_Dir)\n",
        "\n",
        "        #create and open a webpage monitor: (we just replace one line in the html file to update the folder)\n",
        "        with open(\"Classifier/TrainResults/monitor_base.html\") as fin, open(\"Classifier/TrainResults/monitor.html\", 'w') as fout:\n",
        "            for line in fin:\n",
        "                lineout = line\n",
        "                if 'var results_folder' in line:\n",
        "                    lineout = 'var results_folder = \"Classifier/Train_Results_{}/\"'.format(theTime)\n",
        "                fout.write(lineout)\n",
        "\n",
        "        webbrowser.open(\"Classifier/TrainResults/monitor.html\")\n",
        "\n",
        "    def train(self):\n",
        "        # dataset should come as a tuple of (train_dataset,test_dataset)\n",
        "        dataset = download_dataset(self.config, self.transform)\n",
        "        train_data = dataset[0]\n",
        "        test_data  = dataset[1]\n",
        "\n",
        "        # move dataset to dataloader\n",
        "        trainloader = DataLoader(train_data, batch_size=self.config['batch_size'], shuffle=True)\n",
        "        testloader = DataLoader(test_data, batch_size=self.config['batch_size'], shuffle=True)\n",
        "        \n",
        "        # TRAINING CONFIGURATIONS:\n",
        "        net = Net().to(self.device)\n",
        "        print(net)\n",
        "\n",
        "        # loss\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.SGD(net.parameters(), lr=self.config['lr'], momentum=self.config['momentum'])\n",
        "\n",
        "        # define tracking measures:\n",
        "        self.init_tracking_measures()\n",
        "\n",
        "        # apply network changes according to training state\n",
        "        if self.config['state'] == 'MNIST': # if training on MNIST\n",
        "            weights_save_path = self.config['weights_path'] + 'MNIST_weights.pth'\n",
        "\n",
        "        if self.config['state'] == 'HASY': # if training on HASY\n",
        "            weights_load_path = self.config['weights_path'] + 'MNIST_weights.pth'\n",
        "            weights_save_path = self.config['weights_path'] + 'HASY_weights.pth'\n",
        "\n",
        "            net.load_state_dict(torch.load(weights_load_path)['state_dict']) # load MNIST weights\n",
        "            net.fc3 = nn.Linear(84, len(config['sym_list'])).to(self.device)   # change model's last layer\n",
        "\n",
        "\n",
        "        # TRAINING:\n",
        "        print('Start Training on {}'.format(self.device))\n",
        "\n",
        "        for epochNum in range(self.config['train_epochs']):  # no. of epochs\n",
        "\n",
        "            net = self.train_epoch(net, trainloader, epochNum)\n",
        "\n",
        "            self.test_epoch(net, testloader, epochNum)\n",
        "\n",
        "            self.generate_measures_plots() # update figures after each epoch to observe during training\n",
        "\n",
        "        self.save_network(net, weights_save_path)\n",
        "\n",
        "        print('Done Training {} epochs'.format(epochNum+1))\n",
        "\n",
        "        self.generate_measures_plots()\n",
        "\n",
        "    def init_tracking_measures(self):\n",
        "        self.tracking_measures = {}\n",
        "        self.tracking_measures['batch_train_loss'] = []\n",
        "        self.tracking_measures['batch_train_acc'] = []\n",
        "        self.tracking_measures['epoch_train_loss'] = []\n",
        "        self.tracking_measures['epoch_train_acc'] = []\n",
        "        self.tracking_measures['epoch_test_loss'] = []\n",
        "        self.tracking_measures['epoch_test_acc'] = []\n",
        "\n",
        "    def generate_measures_plots(self):\n",
        "        for key, value in self.tracking_measures.items():\n",
        "            fig, ax = plt.subplots(figsize=(10, 5))\n",
        "            ax.plot(value)\n",
        "            ax.set_title(key)\n",
        "            plt.grid()\n",
        "            fn = os.path.join(self.Train_Results_Dir,'{}.png'.format(key))\n",
        "            plt.savefig(fn)\n",
        "            plt.close()\n",
        "\n",
        "    def train_epoch(self, net, trainloader, epoch):\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        net.train()\n",
        "        \n",
        "        for data in tqdm(trainloader):\n",
        "            # data pixels and labels to GPU if available\n",
        "            inputs, labels = data[0].to(self.device, non_blocking=True), data[1].to(self.device, non_blocking=True)\n",
        "            # set the parameter gradients to zero\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            # print(outputs.shape, labels.shape)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            # propagate the loss backward\n",
        "            loss.backward()\n",
        "            # update the gradients\n",
        "            self.optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            epoch_loss += batch_loss\n",
        "            self.tracking_measures['batch_train_loss'].append(batch_loss)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            batch_acc = (predicted == labels).sum().item()/len(predicted)\n",
        "            epoch_acc += batch_acc\n",
        "            self.tracking_measures['batch_train_acc'].append(batch_acc)\n",
        "\n",
        "        epoch_loss /= len(trainloader)\n",
        "        epoch_acc /= len(trainloader)\n",
        "        print('Train Epoch {} loss: {:.3f} acc: {:.3f}'.format(epoch + 1, epoch_loss, epoch_acc))\n",
        "        self.tracking_measures['epoch_train_loss'].append(epoch_loss)\n",
        "        self.tracking_measures['epoch_train_acc'].append(epoch_acc)\n",
        "\n",
        "        return net\n",
        "\n",
        "\n",
        "    def test_epoch(self, net, testloader, epoch):       \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                inputs, labels = data[0].to(self.device, non_blocking=True), data[1].to(self.device, non_blocking=True)\n",
        "                outputs = net(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # total += labels.size(0)\n",
        "                # correct += (predicted == labels).sum().item()\n",
        "\n",
        "                batch_loss = loss.item()\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                batch_acc = (predicted == labels).sum().item()/len(predicted)\n",
        "                epoch_acc += batch_acc\n",
        "\n",
        "        # print('Accuracy of the network on test images: %0.3f %%' % (\n",
        "        #         100 * correct / total))\n",
        "        epoch_loss /= len(testloader)\n",
        "        epoch_acc /= len(testloader)\n",
        "        print('Test Epoch {} loss: {:.3f} acc: {:.3f}'.format(epoch + 1, epoch_loss, epoch_acc))\n",
        "        self.tracking_measures['epoch_test_loss'].append(epoch_loss)\n",
        "        self.tracking_measures['epoch_test_acc'].append(epoch_acc)\n",
        "\n",
        "    def get_device(self):\n",
        "        if torch.cuda.is_available():\n",
        "            device = 'cuda:0'\n",
        "        else:\n",
        "            device = 'cpu'\n",
        "        return device\n",
        "\n",
        "    def save_network(self, net, weights_save_path):\n",
        "        saved_dict = {'state_dict': net.state_dict()}\n",
        "        # add custom data to the saved file:\n",
        "        saved_dict['train_measures'] = self.tracking_measures\n",
        "        saved_dict['config'] = self.config\n",
        "        # saved_dict['class2sym_mapper'] = class2sym_mapper\n",
        "\n",
        "        fn = os.path.join(self.Train_Results_Dir,config['state'] +'.pth')\n",
        "        torch.save(saved_dict, fn)\n",
        "        print('save model in ' + fn)\n",
        "\n",
        "        fn = weights_save_path\n",
        "        torch.save(saved_dict, fn)\n",
        "        print('save model in ' + fn)\n",
        "\n",
        "    def download_dataset(config, transform):\n",
        "      if config['state'] == 'MNIST':\n",
        "        import torchvision\n",
        "        train_dataset = torchvision.datasets.MNIST(config['data_path'], train=True, download=True,\n",
        "                              transform=transform)\n",
        "        test_dataset = torchvision.datasets.MNIST(config['data_path'], train=False, download=True,\n",
        "                              transform=transform)                      \n",
        "\n",
        "\n",
        "      if config['state'] == 'HASY':\n",
        "        if not os.path.exists(config['data_path'] + 'hasy-data'): # download data  \n",
        "          import tarfile\n",
        "          import requests    \n",
        "          url = 'https://zenodo.org/record/259444/files/HASYv2.tar.bz2?download=1'\n",
        "          out = config['data_path'] + 'HASYv2.tar'\n",
        "          print('Downloading HASY dataset')\n",
        "          r = requests.get(url)\n",
        "          with open(out, 'wb') as f:\n",
        "              f.write(r.content)\n",
        "          \n",
        "          my_tar = tarfile.open(out)\n",
        "          print('Extracting dataset')\n",
        "          my_tar.extractall(config['data_path'])  # specify which folder to extract to\n",
        "          my_tar.close()\n",
        "          print('Done extracting')\n",
        "          \n",
        "        meta_data = pd.read_csv(config['data_path'] + 'hasy-data-labels.csv')\n",
        "        # here we concatenate all_df with equal sign df\n",
        "        all_df = mapper(meta_data,config['sym_list']) # slice only needed symbols\n",
        "        print(all_df.latex.value_counts())\n",
        "\n",
        "        dataset = HASYDataset(config,all_df,transform) # read data into dataset\n",
        "        train_size = int(config['HASY_train_split'] * len(dataset))\n",
        "        test_size = len(dataset) - train_size\n",
        "        train_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
        "                                                                    [train_size, test_size]) # split dataset to train and test\n",
        "\n",
        "      return (train_dataset,test_dataset)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAVbBxlP8DXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_dataset(config, transform):\n",
        "    if config['state'] == 'MNIST':\n",
        "      import torchvision\n",
        "      train_dataset = torchvision.datasets.MNIST(config['data_path'], train=True, download=True,\n",
        "                            transform=transform)\n",
        "      test_dataset = torchvision.datasets.MNIST(config['data_path'], train=False, download=True,\n",
        "                            transform=transform)                      \n",
        "\n",
        "\n",
        "    if config['state'] == 'HASY':\n",
        "      if not os.path.exists(config['data_path'] + 'hasy-data'): # download data  \n",
        "        import tarfile\n",
        "        import requests    \n",
        "        url = 'https://zenodo.org/record/259444/files/HASYv2.tar.bz2?download=1'\n",
        "        out = config['data_path'] + 'HASYv2.tar'\n",
        "        print('Downloading HASY dataset')\n",
        "        r = requests.get(url)\n",
        "        with open(out, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        \n",
        "        my_tar = tarfile.open(out)\n",
        "        print('Extracting dataset')\n",
        "        my_tar.extractall(config['data_path'])  # specify which folder to extract to\n",
        "        my_tar.close()\n",
        "        print('Done extracting')\n",
        "        \n",
        "      meta_data = pd.read_csv(config['data_path'] + 'hasy-data-labels.csv')\n",
        "      # here we concatenate all_df with equal sign df\n",
        "      all_df = mapper(meta_data,config['sym_list']) # slice only needed symbols\n",
        "      print(all_df.latex.value_counts())\n",
        "\n",
        "      dataset = HASYDataset(config,all_df,transform) # read data into dataset\n",
        "      train_size = int(config['HASY_train_split'] * len(dataset))\n",
        "      test_size = len(dataset) - train_size\n",
        "      train_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
        "                                                                  [train_size, test_size]) # split dataset to train and test\n",
        "\n",
        "    return (train_dataset,test_dataset)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d-i9rM4yaYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "f19beff4-7055-4e25-8c8c-d59a095ec55e"
      },
      "source": [
        "inner_path = ''\n",
        "config = {}\n",
        "config['inner_path'] = inner_path\n",
        "config['data_path'] = inner_path + 'DataSets/'\n",
        "config['weights_path'] = inner_path + 'Classifier/weights/'\n",
        "config['train_data_path'] = 'classification-task/fold-1/train.csv'\n",
        "config['test_data_path']  = 'classification-task/fold-1/test.csv'\n",
        "config['batch_size'] = 128\n",
        "config['train_epochs'] = 10\n",
        "config['lr'] = 0.01\n",
        "config['momentum'] = 0.9 \n",
        "config['state'] = 'MNIST'\n",
        "config['sym_list'] = ['1','2','3','4','5','6','7','8','9',\n",
        "                      '\\\\alpha','=','+','-','\\\\pi','A','X','\\\\cdot']\n",
        "config['HASY_train_split'] = 0.8\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize(0.5,0.5),\n",
        "                              ])\n",
        "\n",
        "theTrainer = Trainer(config, transform)\n",
        "\n",
        "tic = time.time()\n",
        "theTrainer.train()\n",
        "print('Proccess took {:.2f} m.'.format((time.time() - tic)/60))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|▏         | 6/469 [00:00<00:08, 56.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "Start Training on cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 57.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 1 loss: 0.735 acc: 0.773\n",
            "Test Epoch 1 loss: 0.117 acc: 0.962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 56.53it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 2 loss: 0.092 acc: 0.972\n",
            "Test Epoch 2 loss: 0.063 acc: 0.980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 56.63it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 3 loss: 0.065 acc: 0.980\n",
            "Test Epoch 3 loss: 0.049 acc: 0.984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 56.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 4 loss: 0.050 acc: 0.985\n",
            "Test Epoch 4 loss: 0.043 acc: 0.986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 56.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 5 loss: 0.041 acc: 0.988\n",
            "Test Epoch 5 loss: 0.038 acc: 0.987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 56.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 6 loss: 0.035 acc: 0.989\n",
            "Test Epoch 6 loss: 0.037 acc: 0.987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 54.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 7 loss: 0.032 acc: 0.990\n",
            "Test Epoch 7 loss: 0.035 acc: 0.988\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 55.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 8 loss: 0.027 acc: 0.992\n",
            "Test Epoch 8 loss: 0.029 acc: 0.990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 56.24it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 9 loss: 0.023 acc: 0.993\n",
            "Test Epoch 9 loss: 0.030 acc: 0.989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 56.46it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 10 loss: 0.020 acc: 0.994\n",
            "Test Epoch 10 loss: 0.034 acc: 0.989\n",
            "save model in Classifier/TrainResults/Train_Results_2020-09-01_11-09-06/MNIST.pth\n",
            "save model in Classifier/weights/MNIST_weights.pth\n",
            "Done Training 10 epochs\n",
            "Proccess took 1.71 m.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxu9La5H0l8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58546dfb-19e7-402f-846b-1623a67acceb"
      },
      "source": [
        "config['state'] = 'HASY'\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize([28,28]),                                \n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize(0.5,0.5),\n",
        "                      ])\n",
        "\n",
        "dataset = download_dataset(config, transform)\n",
        "theTrainer = Trainer(config,dataset)\n",
        "\n",
        "tic = time.time()\n",
        "theTrainer.train()\n",
        "print('Proccess took {:.2f} m.'.format((time.time() - tic)/60))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading HASY dataset\n",
            "Extracting dataset\n",
            "Done extracting\n",
            "\\alpha    2601\n",
            "\\pi       1533\n",
            "\\cdot      755\n",
            "A          159\n",
            "2          124\n",
            "8          121\n",
            "3          120\n",
            "1          118\n",
            "-          118\n",
            "6          100\n",
            "9           90\n",
            "+           90\n",
            "5           78\n",
            "7           75\n",
            "4           61\n",
            "X           54\n",
            "Name: latex, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/39 [00:00<00:02, 15.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "Start Training on cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 16.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 1 loss: 1.307 acc: 0.682\n",
            "Test Epoch 1 loss: 0.849 acc: 0.749\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 19.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 2 loss: 0.627 acc: 0.811\n",
            "Test Epoch 2 loss: 0.508 acc: 0.874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 3 loss: 0.378 acc: 0.896\n",
            "Test Epoch 3 loss: 0.368 acc: 0.906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 4 loss: 0.276 acc: 0.925\n",
            "Test Epoch 4 loss: 0.288 acc: 0.930\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:01<00:00, 19.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 5 loss: 0.218 acc: 0.943\n",
            "Test Epoch 5 loss: 0.255 acc: 0.939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 6 loss: 0.182 acc: 0.953\n",
            "Test Epoch 6 loss: 0.242 acc: 0.945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 7 loss: 0.152 acc: 0.958\n",
            "Test Epoch 7 loss: 0.212 acc: 0.947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 19.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 8 loss: 0.123 acc: 0.968\n",
            "Test Epoch 8 loss: 0.209 acc: 0.951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 9 loss: 0.115 acc: 0.968\n",
            "Test Epoch 9 loss: 0.190 acc: 0.955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 10 loss: 0.097 acc: 0.973\n",
            "Test Epoch 10 loss: 0.204 acc: 0.949\n",
            "save model in Classifier/TrainResults/Train_Results_2020-09-01_10-58-47/HASY.pth\n",
            "save model in Classifier/weights/HASY_weights.pth\n",
            "Done Training 10 epochs\n",
            "Proccess took 0.54 m.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhTv2ClV4556",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git add '/content/EquSolve/Classifier/TrainResults/Train_Results_2020-08-27_11-28-41\n",
        "# !git commit --message=\"Add train results\"'\n",
        "# !git push --set-upstream origin master"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_xZEZvW-9-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git remote rm origin\n",
        "# !git remote add origin 'git@github.com:yuvalgrossman/EquSolve.git'\n",
        "# # !RUN git clone https://github.com/edenhill/librdkafka.git"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}