{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HASY_trainer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNj8slWWEdtR9eeIZOQuAvO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvalgrossman/EquSolve/blob/master/Classifier/HASY_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAqqBRMGYo0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "038188f7-470b-4916-a382-414c0b271519"
      },
      "source": [
        "import os\n",
        "%cd '/content/'\n",
        "if not os.path.isdir('EquSolve'):\n",
        "  !git clone http://github.com/yuvalgrossman/EquSolve\n",
        "  %cd EquSolve\n",
        "else:\n",
        "  %cd EquSolve\n",
        "  !git pull\n",
        "\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "from tqdm import tqdm\n",
        "import webbrowser\n",
        "import time\n",
        "import pdb\n",
        "\n",
        "# project classes:\n",
        "from Classifier.HASYDataset import HASYDataset\n",
        "from Classifier.Net import Net\n",
        "from Utils.mapper import mapper"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/EquSolve\n",
            "warning: redirecting to https://github.com/yuvalgrossman/EquSolve/\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 9 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n",
            "From http://github.com/yuvalgrossman/EquSolve\n",
            "   9b1024f..b798d58  master     -> origin/master\n",
            "Updating 9b1024f..b798d58\n",
            "error: The following untracked working tree files would be overwritten by merge:\n",
            "\tClassifier/weights/HASY_weights.pth\n",
            "Please move or remove them before you merge.\n",
            "Aborting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdLVN95gtHvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, config, transform):\n",
        "        self.config = config\n",
        "        self.device = self.get_device()\n",
        "        self.transform = transform\n",
        "\n",
        "        #create dir to save train results:\n",
        "        theTime = \"{date:%Y-%m-%d_%H-%M-%S}\".format(date=datetime.datetime.now())\n",
        "        self.Train_Results_Dir = 'Classifier/TrainResults/Train_Results_' + theTime\n",
        "        os.mkdir(self.Train_Results_Dir)\n",
        "\n",
        "        #create and open a webpage monitor: (we just replace one line in the html file to update the folder)\n",
        "        with open(\"Classifier/TrainResults/monitor_base.html\") as fin, open(\"Classifier/TrainResults/monitor.html\", 'w') as fout:\n",
        "            for line in fin:\n",
        "                lineout = line\n",
        "                if 'var results_folder' in line:\n",
        "                    lineout = 'var results_folder = \"Classifier/Train_Results_{}/\"'.format(theTime)\n",
        "                fout.write(lineout)\n",
        "\n",
        "        webbrowser.open(\"Classifier/TrainResults/monitor.html\")\n",
        "\n",
        "    def train(self):\n",
        "        # dataset should come as a tuple of (train_dataset,test_dataset)\n",
        "        dataset = download_dataset(self.config, self.transform)\n",
        "        train_data = dataset[0]\n",
        "        test_data  = dataset[1]\n",
        "\n",
        "        # move dataset to dataloader\n",
        "        trainloader = DataLoader(train_data, batch_size=self.config['batch_size'], shuffle=True)\n",
        "        testloader = DataLoader(test_data, batch_size=self.config['batch_size'], shuffle=True)\n",
        "        \n",
        "        # TRAINING CONFIGURATIONS:\n",
        "        net = Net().to(self.device)\n",
        "        print(net)\n",
        "\n",
        "        # loss\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.SGD(net.parameters(), lr=self.config['lr'], momentum=self.config['momentum'])\n",
        "\n",
        "        # define tracking measures:\n",
        "        self.init_tracking_measures()\n",
        "\n",
        "        # apply network changes according to training state\n",
        "        if self.config['state'] == 'MNIST': # if training on MNIST\n",
        "            weights_save_path = self.config['weights_path'] + 'MNIST_weights.pth'\n",
        "\n",
        "        if self.config['state'] == 'HASY': # if training on HASY\n",
        "            weights_load_path = self.config['weights_path'] + 'MNIST_weights.pth'\n",
        "            weights_save_path = self.config['weights_path'] + 'HASY_weights.pth'\n",
        "\n",
        "            net.load_state_dict(torch.load(weights_load_path)['state_dict']) # load MNIST weights\n",
        "            net.fc3 = nn.Linear(84, len(config['sym_list'])).to(self.device)   # change model's last layer\n",
        "\n",
        "\n",
        "        # TRAINING:\n",
        "        print('Start Training on {}'.format(self.device))\n",
        "\n",
        "        for epochNum in range(self.config['train_epochs']):  # no. of epochs\n",
        "\n",
        "            net = self.train_epoch(net, trainloader, epochNum)\n",
        "\n",
        "            self.test_epoch(net, testloader, epochNum)\n",
        "\n",
        "            self.generate_measures_plots() # update figures after each epoch to observe during training\n",
        "\n",
        "        self.save_network(net, weights_save_path)\n",
        "\n",
        "        print('Done Training {} epochs'.format(epochNum+1))\n",
        "\n",
        "        self.generate_measures_plots()\n",
        "\n",
        "    def init_tracking_measures(self):\n",
        "        self.tracking_measures = {}\n",
        "        self.tracking_measures['batch_train_loss'] = []\n",
        "        self.tracking_measures['batch_train_acc'] = []\n",
        "        self.tracking_measures['epoch_train_loss'] = []\n",
        "        self.tracking_measures['epoch_train_acc'] = []\n",
        "        self.tracking_measures['epoch_test_loss'] = []\n",
        "        self.tracking_measures['epoch_test_acc'] = []\n",
        "\n",
        "    def generate_measures_plots(self):\n",
        "        for key, value in self.tracking_measures.items():\n",
        "            fig, ax = plt.subplots(figsize=(10, 5))\n",
        "            ax.plot(value)\n",
        "            ax.set_title(key)\n",
        "            plt.grid()\n",
        "            fn = os.path.join(self.Train_Results_Dir,'{}.png'.format(key))\n",
        "            plt.savefig(fn)\n",
        "            plt.close()\n",
        "\n",
        "    def train_epoch(self, net, trainloader, epoch):\n",
        "\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        net.train()\n",
        "        \n",
        "        for data in tqdm(trainloader):\n",
        "            # data pixels and labels to GPU if available\n",
        "            inputs, labels = data[0].to(self.device, non_blocking=True), data[1].to(self.device, non_blocking=True)\n",
        "            # set the parameter gradients to zero\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            # print(outputs.shape, labels.shape)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            # propagate the loss backward\n",
        "            loss.backward()\n",
        "            # update the gradients\n",
        "            self.optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            epoch_loss += batch_loss\n",
        "            self.tracking_measures['batch_train_loss'].append(batch_loss)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            batch_acc = (predicted == labels).sum().item()/len(predicted)\n",
        "            epoch_acc += batch_acc\n",
        "            self.tracking_measures['batch_train_acc'].append(batch_acc)\n",
        "\n",
        "        epoch_loss /= len(trainloader)\n",
        "        epoch_acc /= len(trainloader)\n",
        "        print('Train Epoch {} loss: {:.3f} acc: {:.3f}'.format(epoch + 1, epoch_loss, epoch_acc))\n",
        "        self.tracking_measures['epoch_train_loss'].append(epoch_loss)\n",
        "        self.tracking_measures['epoch_train_acc'].append(epoch_acc)\n",
        "\n",
        "        return net\n",
        "\n",
        "\n",
        "    def test_epoch(self, net, testloader, epoch):       \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            for data in testloader:\n",
        "                inputs, labels = data[0].to(self.device, non_blocking=True), data[1].to(self.device, non_blocking=True)\n",
        "                outputs = net(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # total += labels.size(0)\n",
        "                # correct += (predicted == labels).sum().item()\n",
        "\n",
        "                batch_loss = loss.item()\n",
        "                epoch_loss += batch_loss\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                batch_acc = (predicted == labels).sum().item()/len(predicted)\n",
        "                epoch_acc += batch_acc\n",
        "\n",
        "        # print('Accuracy of the network on test images: %0.3f %%' % (\n",
        "        #         100 * correct / total))\n",
        "        epoch_loss /= len(testloader)\n",
        "        epoch_acc /= len(testloader)\n",
        "        print('Test Epoch {} loss: {:.3f} acc: {:.3f}'.format(epoch + 1, epoch_loss, epoch_acc))\n",
        "        self.tracking_measures['epoch_test_loss'].append(epoch_loss)\n",
        "        self.tracking_measures['epoch_test_acc'].append(epoch_acc)\n",
        "\n",
        "    def get_device(self):\n",
        "        if torch.cuda.is_available():\n",
        "            device = 'cuda:0'\n",
        "        else:\n",
        "            device = 'cpu'\n",
        "        return device\n",
        "\n",
        "    def save_network(self, net, weights_save_path):\n",
        "        saved_dict = {'state_dict': net.state_dict()}\n",
        "        # add custom data to the saved file:\n",
        "        saved_dict['train_measures'] = self.tracking_measures\n",
        "        saved_dict['config'] = self.config\n",
        "        # saved_dict['class2sym_mapper'] = class2sym_mapper\n",
        "\n",
        "        fn = os.path.join(self.Train_Results_Dir,config['state'] +'.pth')\n",
        "        torch.save(saved_dict, fn)\n",
        "        print('save model in ' + fn)\n",
        "\n",
        "        fn = weights_save_path\n",
        "        torch.save(saved_dict, fn)\n",
        "        print('save model in ' + fn)\n",
        "\n",
        "    def download_dataset(config, transform):\n",
        "      if config['state'] == 'MNIST':\n",
        "        import torchvision\n",
        "        train_dataset = torchvision.datasets.MNIST(config['data_path'], train=True, download=True,\n",
        "                              transform=transform)\n",
        "        test_dataset = torchvision.datasets.MNIST(config['data_path'], train=False, download=True,\n",
        "                              transform=transform)                      \n",
        "\n",
        "\n",
        "      if config['state'] == 'HASY':\n",
        "        if not os.path.exists(config['data_path'] + 'hasy-data'): # download data  \n",
        "          import tarfile\n",
        "          import requests    \n",
        "          url = 'https://zenodo.org/record/259444/files/HASYv2.tar.bz2?download=1'\n",
        "          out = config['data_path'] + 'HASYv2.tar'\n",
        "          print('Downloading HASY dataset')\n",
        "          r = requests.get(url)\n",
        "          with open(out, 'wb') as f:\n",
        "              f.write(r.content)\n",
        "          \n",
        "          my_tar = tarfile.open(out)\n",
        "          print('Extracting dataset')\n",
        "          my_tar.extractall(config['data_path'])  # specify which folder to extract to\n",
        "          my_tar.close()\n",
        "          print('Done extracting')\n",
        "          \n",
        "        meta_data = pd.read_csv(config['data_path'] + 'hasy-data-labels.csv')\n",
        "        # here we concatenate all_df with equal sign df\n",
        "        all_df = mapper(meta_data,config['sym_list']) # slice only needed symbols\n",
        "        print(all_df.latex.value_counts())\n",
        "\n",
        "        dataset = HASYDataset(config,all_df,transform) # read data into dataset\n",
        "        train_size = int(config['HASY_train_split'] * len(dataset))\n",
        "        test_size = len(dataset) - train_size\n",
        "        train_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
        "                                                                    [train_size, test_size]) # split dataset to train and test\n",
        "\n",
        "      return (train_dataset,test_dataset)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAVbBxlP8DXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_dataset(config, transform):\n",
        "    if config['state'] == 'MNIST':\n",
        "      import torchvision\n",
        "      train_dataset = torchvision.datasets.MNIST(config['data_path'], train=True, download=True,\n",
        "                            transform=transform)\n",
        "      test_dataset = torchvision.datasets.MNIST(config['data_path'], train=False, download=True,\n",
        "                            transform=transform)                      \n",
        "\n",
        "\n",
        "    if config['state'] == 'HASY':\n",
        "      if not os.path.exists(config['data_path'] + 'hasy-data'): # download data  \n",
        "        import tarfile\n",
        "        import requests    \n",
        "        url = 'https://zenodo.org/record/259444/files/HASYv2.tar.bz2?download=1'\n",
        "        out = config['data_path'] + 'HASYv2.tar'\n",
        "        print('Downloading HASY dataset')\n",
        "        r = requests.get(url)\n",
        "        with open(out, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        \n",
        "        my_tar = tarfile.open(out)\n",
        "        print('Extracting dataset')\n",
        "        my_tar.extractall(config['data_path'])  # specify which folder to extract to\n",
        "        my_tar.close()\n",
        "        print('Done extracting')\n",
        "        \n",
        "      meta_data = pd.read_csv(config['data_path'] + 'hasy-data-labels.csv')\n",
        "      # here we concatenate all_df with equal sign df\n",
        "      all_df = mapper(meta_data,config['sym_list']) # slice only needed symbols\n",
        "      print(all_df.latex.value_counts())\n",
        "\n",
        "      dataset = HASYDataset(config,all_df,transform) # read data into dataset\n",
        "      train_size = int(config['HASY_train_split'] * len(dataset))\n",
        "      test_size = len(dataset) - train_size\n",
        "      train_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
        "                                                                  [train_size, test_size]) # split dataset to train and test\n",
        "\n",
        "    return (train_dataset,test_dataset)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d-i9rM4yaYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "7167dd2e-f816-41c8-d7f6-22242442a897"
      },
      "source": [
        "inner_path = ''\n",
        "config = {}\n",
        "config['inner_path'] = inner_path\n",
        "config['data_path'] = inner_path + 'DataSets/'\n",
        "config['weights_path'] = inner_path + 'Classifier/weights/'\n",
        "config['train_data_path'] = 'classification-task/fold-1/train.csv'\n",
        "config['test_data_path']  = 'classification-task/fold-1/test.csv'\n",
        "config['batch_size'] = 128\n",
        "config['train_epochs'] = 5\n",
        "config['lr'] = 0.01\n",
        "config['momentum'] = 0.9 \n",
        "config['state'] = 'MNIST'\n",
        "config['sym_list'] = ['1','2','3','4','5','6','7','8','9',\n",
        "                      '\\\\alpha','=','+','-','\\\\pi','A','X','\\\\cdot']\n",
        "config['HASY_train_split'] = 0.8\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize(0.5,0.5),\n",
        "                              ])\n",
        "\n",
        "theTrainer = Trainer(config, transform)\n",
        "\n",
        "tic = time.time()\n",
        "theTrainer.train()\n",
        "print('Proccess took {:.2f} m.'.format((time.time() - tic)/60))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|▏         | 6/469 [00:00<00:08, 52.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "Start Training on cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 55.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 1 loss: 0.659 acc: 0.778\n",
            "Test Epoch 1 loss: 0.114 acc: 0.966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 55.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 2 loss: 0.091 acc: 0.972\n",
            "Test Epoch 2 loss: 0.078 acc: 0.975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 55.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 3 loss: 0.064 acc: 0.980\n",
            "Test Epoch 3 loss: 0.042 acc: 0.987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 55.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 4 loss: 0.052 acc: 0.984\n",
            "Test Epoch 4 loss: 0.043 acc: 0.986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:08<00:00, 55.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 5 loss: 0.043 acc: 0.986\n",
            "Test Epoch 5 loss: 0.037 acc: 0.988\n",
            "save model in Classifier/TrainResults/Train_Results_2020-09-01_11-12-39/MNIST.pth\n",
            "save model in Classifier/weights/MNIST_weights.pth\n",
            "Done Training 5 epochs\n",
            "Proccess took 0.93 m.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxu9La5H0l8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cbd5141a-2106-4e34-f32d-08c7ac8ee226"
      },
      "source": [
        "config['state'] = 'HASY'\n",
        "config['train_epochs'] = 10\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize([28,28]),                                \n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize(0.5,0.5),\n",
        "                      ])\n",
        "\n",
        "theTrainer = Trainer(config, transform)\n",
        "\n",
        "tic = time.time()\n",
        "theTrainer.train()\n",
        "print('Proccess took {:.2f} m.'.format((time.time() - tic)/60))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/39 [00:00<00:02, 18.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\\alpha    2601\n",
            "\\pi       1533\n",
            "\\cdot      755\n",
            "A          159\n",
            "2          124\n",
            "8          121\n",
            "3          120\n",
            "-          118\n",
            "1          118\n",
            "6          100\n",
            "+           90\n",
            "9           90\n",
            "5           78\n",
            "7           75\n",
            "4           61\n",
            "X           54\n",
            "Name: latex, dtype: int64\n",
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "Start Training on cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.48it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 1 loss: 1.472 acc: 0.612\n",
            "Test Epoch 1 loss: 0.837 acc: 0.764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 2 loss: 0.645 acc: 0.819\n",
            "Test Epoch 2 loss: 0.510 acc: 0.867\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 3 loss: 0.390 acc: 0.900\n",
            "Test Epoch 3 loss: 0.337 acc: 0.907\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.88it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 4 loss: 0.265 acc: 0.928\n",
            "Test Epoch 4 loss: 0.273 acc: 0.925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 5 loss: 0.206 acc: 0.944\n",
            "Test Epoch 5 loss: 0.221 acc: 0.935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 6 loss: 0.167 acc: 0.956\n",
            "Test Epoch 6 loss: 0.206 acc: 0.941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 19.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 7 loss: 0.147 acc: 0.963\n",
            "Test Epoch 7 loss: 0.180 acc: 0.946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 8 loss: 0.128 acc: 0.966\n",
            "Test Epoch 8 loss: 0.168 acc: 0.954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 9 loss: 0.116 acc: 0.969\n",
            "Test Epoch 9 loss: 0.163 acc: 0.951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 39/39 [00:02<00:00, 18.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch 10 loss: 0.100 acc: 0.973\n",
            "Test Epoch 10 loss: 0.154 acc: 0.957\n",
            "save model in Classifier/TrainResults/Train_Results_2020-09-01_11-17-03/HASY.pth\n",
            "save model in Classifier/weights/HASY_weights.pth\n",
            "Done Training 10 epochs\n",
            "Proccess took 0.55 m.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhTv2ClV4556",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git add '/content/EquSolve/Classifier/TrainResults/Train_Results_2020-08-27_11-28-41\n",
        "# !git commit --message=\"Add train results\"'\n",
        "# !git push --set-upstream origin master"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_xZEZvW-9-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git remote rm origin\n",
        "# !git remote add origin 'git@github.com:yuvalgrossman/EquSolve.git'\n",
        "# # !RUN git clone https://github.com/edenhill/librdkafka.git"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}